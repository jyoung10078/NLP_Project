{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples of some stop words in English\n",
    "stopwords.words('english')[0:1000:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                    v2   \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \\\n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "  Unnamed: 2 Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN        NaN  \n",
       "1        NaN        NaN        NaN  \n",
       "2        NaN        NaN        NaN  \n",
       "3        NaN        NaN        NaN  \n",
       "4        NaN        NaN        NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Pulling in the data and looking at the top rows\n",
    "messages = pd.read_csv('/Users/jared/Downloads/Ex_Files_Adv_NLP_Python_ML/Exercise Files/data/spam.csv', encoding = 'latin-1')\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean up the data\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary stats on our data\n",
    "messages.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a lot more ham values than ham values\n",
    "messages['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in label: 0\n",
      "Number of nulls in text: 0\n"
     ]
    }
   ],
   "source": [
    "# Missing Data\n",
    "print('Number of nulls in label: {}'.format(messages['label'].isnull().sum()))\n",
    "print('Number of nulls in text: {}'.format(messages['text'].isnull().sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Changing the way the tables will be displayed so that we can read more of the data this time around\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "\n",
    "#To do this, we need to show python what punctuation looks like\n",
    "#This library has a package called \"puncatuation\" that we can use for this step\n",
    "import string\n",
    "\n",
    "#Showing the punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The reason we do this is to remove noise from the data\n",
    "\"This message is spam\" == \"This message is spam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text   \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \\\n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...  \n",
       "1                                                                              Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "3                                                          U dun say so early hor U c already then say  \n",
       "4                                          Nah I dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building a function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: remove_punct(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text   \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \\\n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean   \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...  \\\n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                        text_tokenized  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization - splitting our sentences into a list of words\n",
    "import re\n",
    "\n",
    "# \\W+ will split a text wherever it sees one or more non-word characters (white space, special characters, etc.)\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# We apply our function and then lower case all our words because python is case sensitive\n",
    "messages['text_tokenized'] = messages['text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text   \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \\\n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean   \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...  \\\n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                        text_tokenized   \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...  \\\n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                           text_nostop  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define a function to remove the stop words\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "messages['text_nostop'] = messages['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "- This creates a document-term matrix; one row per document, one column per word in the corpus\n",
    "- Generates a weighting for each word/document pair intended to reflect how important a given word is to the document within the context of its frequency within a larger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Reformat Data\n",
    "mess = pd.read_csv('/Users/jared/Downloads/Ex_Files_Adv_NLP_Python_ML/Exercise Files/data/spam.csv', encoding = 'latin-1')\n",
    "mess = mess.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "mess.columns = [\"label\", \"text\"]\n",
    "mess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One function to pre-process the data\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 9395)\n",
      "['' '0' '008704050406' ... 'ûïharry' 'ûò' 'ûówell']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9385</th>\n",
       "      <th>9386</th>\n",
       "      <th>9387</th>\n",
       "      <th>9388</th>\n",
       "      <th>9389</th>\n",
       "      <th>9390</th>\n",
       "      <th>9391</th>\n",
       "      <th>9392</th>\n",
       "      <th>9393</th>\n",
       "      <th>9394</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  9385   \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0  \\\n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "   9386  9387  9388  9389  9390  9391  9392  9393  9394  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 9395 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting a basic TFIDF Vectorizer and view the results\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#This will clean the data, fit it in a vectorizer, then create our document form matrix\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(mess['text'])\n",
    "\n",
    "#Output\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "#Feature Names\n",
    "print(tfidf_vect.get_feature_names_out())\n",
    "\n",
    "#Converting the sparse matrix to dataframe\n",
    "X_features = pd.DataFrame(X_tfidf.toarray())\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying a machine learning \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the methods that will be needed to evaluate a basic model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features,\n",
    "                                                    messages['label'],\n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a basic Random Forest Model\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.796\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model predictions using precision and recall\n",
    "precision = precision_score(y_test, y_pred, pos_label = 'spam')\n",
    "recall = recall_score(y_test, y_pred, pos_label = 'spam')\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3), round(recall,3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- Word2Vec is a shallow, two-layer neural network that accepts a text corpus as an input, and it returns a set of vectors (also known as embeddings); each vector is a numberic representation of a given word.\n",
    "- \"You shall know a word by the company it keeps.\"\n",
    "- Python learns the context of a word by looking at a window of words before and after it in the corpus. (Skip gramm method)\n",
    "- When we convert all the words in our corpus into a vector representation, we can then graph each vector. This gives us the ability to see how similar two words are (in meaning) by finding their \"cosine similarity\" (cosine of the two angles of the two vectors).\n",
    "- Theorectically, you can construct analogies with these vectors. For example, if you subtracts man from the vector \"King\" and then added \"woman\" you would get the vector for \"Queen\". Thus, the model could create the analogy \"A man is to king as a woman is to queen\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other pre-trained embeddings to explore\n",
    "- glove-twitter-{25/50/100/200}\n",
    "- glove-wiki-gigaword-{50/200/300}\n",
    "- word2vec-google-news-300\n",
    "- word2vec-ruscorpora-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "#Installing Gensim\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained word vectors using gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the word vector for \"king\"\n",
    "wiki_embeddings['king']\n",
    "\n",
    "# This is a numeric representation the word \"king\". Using the cosine similarity technique, we can find vectors that are the most similar to the vector for \"king\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ideals', 0.6479806303977966),\n",
       " ('devotion', 0.6334191560745239),\n",
       " ('belief', 0.6317397952079773),\n",
       " ('morality', 0.6111955046653748),\n",
       " ('discipline', 0.6109753251075745),\n",
       " ('virtues', 0.6106486916542053),\n",
       " ('true', 0.6067261099815369),\n",
       " ('patriotism', 0.6050897836685181),\n",
       " ('moral', 0.5999709963798523),\n",
       " ('necessity', 0.5992565751075745)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding similar vectors\n",
    "wiki_embeddings.most_similar('virtue')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and clean up column names\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "\n",
    "messages = pd.read_csv('/Users/jared/Downloads/Ex_Files_Adv_NLP_Python_ML/Exercise Files/data/spam.csv', encoding = 'latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   \n",
       "0   ham  \\\n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text   \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \\\n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...  \n",
       "1                                                                          [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "3                                                       [dun, say, so, early, hor, already, then, say]  \n",
       "4                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using gensim's built-in data pre-processor\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for training purposes\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   window=5,\n",
    "                                   min_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01501366,  0.02767842,  0.0081264 ,  0.00025378,  0.01012371,\n",
       "       -0.06689952,  0.02671312,  0.08102237, -0.0200811 , -0.05020381,\n",
       "       -0.01313363, -0.06205527, -0.00835234,  0.01877814,  0.00205935,\n",
       "       -0.01102802,  0.02242907, -0.03060235, -0.00116967, -0.09444411,\n",
       "        0.01765111,  0.00886815,  0.01604872, -0.03018015, -0.01928473,\n",
       "        0.00773258, -0.04124585, -0.0270099 , -0.0254945 ,  0.00595166,\n",
       "        0.0337431 ,  0.0068646 ,  0.01580341, -0.04091191, -0.01823169,\n",
       "        0.05437686,  0.00846641, -0.03355348, -0.00264585, -0.07072156,\n",
       "        0.0035527 , -0.03916473, -0.03678926,  0.00590911,  0.05124301,\n",
       "       -0.00368593, -0.03757008, -0.0146032 ,  0.01419565,  0.02798258,\n",
       "        0.02450917, -0.02944633, -0.00941287,  0.01597572, -0.00533353,\n",
       "        0.01565976,  0.01492448,  0.01272453, -0.01411441,  0.01342273,\n",
       "        0.02347603,  0.01636804, -0.00064177, -0.00951722, -0.05886045,\n",
       "        0.04269448,  0.00506825,  0.03498377, -0.04760833,  0.04964571,\n",
       "       -0.01743008,  0.03168404,  0.04207058, -0.00961159,  0.03254186,\n",
       "        0.00098954,  0.00479467, -0.01338354, -0.04424987,  0.00828983,\n",
       "       -0.01530515,  0.00057627, -0.04223497,  0.05435565, -0.01332433,\n",
       "        0.00169829,  0.00296051,  0.03345089,  0.05214642,  0.01729364,\n",
       "        0.04567148,  0.01702904,  0.01190164,  0.01864011,  0.06815207,\n",
       "        0.02883142,  0.01257151, -0.02376439,  0.02951268, -0.00284072],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the word vecotr for \"king\" base on our trained model\n",
    "w2v_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kind', 0.9858094453811646),\n",
       " ('voucher', 0.9847893118858337),\n",
       " ('kinda', 0.9847197532653809),\n",
       " ('probably', 0.9846299290657043),\n",
       " ('hope', 0.9845755696296692),\n",
       " ('are', 0.9845169186592102),\n",
       " ('good', 0.9845061302185059),\n",
       " ('says', 0.9844889640808105),\n",
       " ('looking', 0.9844638109207153),\n",
       " ('too', 0.9844145178794861)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See most similar words to \"king\" based on word vectors from our trained model\n",
    "w2v_model.wv.most_similar('king')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'to',\n",
       " 'the',\n",
       " 'and',\n",
       " 'is',\n",
       " 'in',\n",
       " 'me',\n",
       " 'my',\n",
       " 'for',\n",
       " 'it',\n",
       " 'your',\n",
       " 'call',\n",
       " 'of',\n",
       " 'that',\n",
       " 'have',\n",
       " 'on',\n",
       " 'now',\n",
       " 'are',\n",
       " 'so',\n",
       " 'can',\n",
       " 'not',\n",
       " 'but',\n",
       " 'or',\n",
       " 'at',\n",
       " 'we',\n",
       " 'get',\n",
       " 'be',\n",
       " 'do',\n",
       " 'with',\n",
       " 'no',\n",
       " 'just',\n",
       " 'if',\n",
       " 'ur',\n",
       " 'will',\n",
       " 'this',\n",
       " 'up',\n",
       " 'free',\n",
       " 'how',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'when',\n",
       " 'from',\n",
       " 'what',\n",
       " 'go',\n",
       " 'all',\n",
       " 'll',\n",
       " 'out',\n",
       " 'ok',\n",
       " 'know',\n",
       " 'am',\n",
       " 'like',\n",
       " 'day',\n",
       " 'was',\n",
       " 'then',\n",
       " 'got',\n",
       " 'he',\n",
       " 'good',\n",
       " 'its',\n",
       " 'come',\n",
       " 'only',\n",
       " 'there',\n",
       " 'time',\n",
       " 'want',\n",
       " 'love',\n",
       " 'send',\n",
       " 'text',\n",
       " 'she',\n",
       " 'today',\n",
       " 'txt',\n",
       " 'as',\n",
       " 'stop',\n",
       " 'by',\n",
       " 'one',\n",
       " 'going',\n",
       " 'sorry',\n",
       " 'mobile',\n",
       " 'don',\n",
       " 'home',\n",
       " 'our',\n",
       " 'about',\n",
       " 'still',\n",
       " 'lor',\n",
       " 'see',\n",
       " 'hi',\n",
       " 'need',\n",
       " 'reply',\n",
       " 'back',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'take',\n",
       " 'they',\n",
       " 'later',\n",
       " 'new',\n",
       " 'please',\n",
       " 'pls',\n",
       " 'any',\n",
       " 'been',\n",
       " 'da',\n",
       " 'some',\n",
       " 'week',\n",
       " 'did',\n",
       " 'dont',\n",
       " 'ì_',\n",
       " 'think',\n",
       " 'has',\n",
       " 'here',\n",
       " 'hope',\n",
       " 'great',\n",
       " 'too',\n",
       " 'where',\n",
       " 'phone',\n",
       " 'dear',\n",
       " 'well',\n",
       " 'night',\n",
       " 'msg',\n",
       " 're',\n",
       " 'him',\n",
       " 'who',\n",
       " 'much',\n",
       " 'won',\n",
       " 'claim',\n",
       " 'more',\n",
       " 'an',\n",
       " 'happy',\n",
       " 'hey',\n",
       " 'oh',\n",
       " 'way',\n",
       " 'had',\n",
       " 'prize',\n",
       " 'make',\n",
       " 'already',\n",
       " 'work',\n",
       " 'number',\n",
       " 'wat',\n",
       " 've',\n",
       " 'give',\n",
       " 'say',\n",
       " 'yes',\n",
       " 'doing',\n",
       " 'should',\n",
       " 'www',\n",
       " 'tomorrow',\n",
       " 'cash',\n",
       " 'after',\n",
       " 'why',\n",
       " 'would',\n",
       " 'yeah',\n",
       " 'them',\n",
       " 'ask',\n",
       " 'im',\n",
       " 'find',\n",
       " 'really',\n",
       " 'said',\n",
       " 'message',\n",
       " 'very',\n",
       " 'babe',\n",
       " 'morning',\n",
       " 'nokia',\n",
       " 'cos',\n",
       " 'miss',\n",
       " 'lol',\n",
       " 'win',\n",
       " 'thanks',\n",
       " 'right',\n",
       " 'sure',\n",
       " 'life',\n",
       " 'meet',\n",
       " 'urgent',\n",
       " 'sent',\n",
       " 'amp',\n",
       " 'last',\n",
       " 'care',\n",
       " 'over',\n",
       " 'something',\n",
       " 'anything',\n",
       " 'were',\n",
       " 'uk',\n",
       " 'com',\n",
       " 'tonight',\n",
       " 'every',\n",
       " 'place',\n",
       " 'pick',\n",
       " 'also',\n",
       " 'let',\n",
       " 'keep',\n",
       " 'before',\n",
       " 'gonna',\n",
       " 'service',\n",
       " 'wait',\n",
       " 'went',\n",
       " 'thing',\n",
       " 'contact',\n",
       " 'ya',\n",
       " 'buy',\n",
       " 'us',\n",
       " 'someone',\n",
       " 'customer',\n",
       " 'sms',\n",
       " 'again',\n",
       " 'soon',\n",
       " 'min',\n",
       " 'around',\n",
       " 'his',\n",
       " 'next',\n",
       " 'other',\n",
       " 'first',\n",
       " 'cant',\n",
       " 'help',\n",
       " 'dun',\n",
       " 'late',\n",
       " 'box',\n",
       " 'leave',\n",
       " 'nice',\n",
       " 'feel',\n",
       " 'per',\n",
       " 'many',\n",
       " 'money',\n",
       " 'hello',\n",
       " 'waiting',\n",
       " 'told',\n",
       " 'wish',\n",
       " 'which',\n",
       " 'off',\n",
       " 'mins',\n",
       " 'chat',\n",
       " 'down',\n",
       " 'fine',\n",
       " 'gud',\n",
       " 'pm',\n",
       " 'friend',\n",
       " 'even',\n",
       " 'co',\n",
       " 'yet',\n",
       " 'thk',\n",
       " 'name',\n",
       " 'friends',\n",
       " 'could',\n",
       " 'sleep',\n",
       " 'coming',\n",
       " 'try',\n",
       " 'special',\n",
       " 'ppm',\n",
       " 'thought',\n",
       " 'best',\n",
       " 'man',\n",
       " 'yup',\n",
       " 'getting',\n",
       " 'guaranteed',\n",
       " 'haha',\n",
       " 'same',\n",
       " 'use',\n",
       " 'wan',\n",
       " 'tone',\n",
       " 'holiday',\n",
       " 'st',\n",
       " 'being',\n",
       " 'may',\n",
       " 'things',\n",
       " 'stuff',\n",
       " 'line',\n",
       " 'done',\n",
       " 'better',\n",
       " 'always',\n",
       " 'days',\n",
       " 'th',\n",
       " 'enjoy',\n",
       " 'job',\n",
       " 'ìï',\n",
       " 'year',\n",
       " 'than',\n",
       " 'camera',\n",
       " 'people',\n",
       " 'ill',\n",
       " 'finish',\n",
       " 'cs',\n",
       " 'talk',\n",
       " 'live',\n",
       " 'nothing',\n",
       " 'pobox',\n",
       " 'dat',\n",
       " 'real',\n",
       " 'because',\n",
       " 'problem',\n",
       " 'person',\n",
       " 'smile',\n",
       " 'another',\n",
       " 'shit',\n",
       " 'lunch',\n",
       " 'ready',\n",
       " 'lar',\n",
       " 'few',\n",
       " 'bit',\n",
       " 'class',\n",
       " 'meeting',\n",
       " 'half',\n",
       " 'wanna',\n",
       " 'yo',\n",
       " 'end',\n",
       " 'heart',\n",
       " 'account',\n",
       " 'eat',\n",
       " 'might',\n",
       " 'god',\n",
       " 'draw',\n",
       " 'thats',\n",
       " 'po',\n",
       " 'trying',\n",
       " 'car',\n",
       " 'check',\n",
       " 'dinner',\n",
       " 'birthday',\n",
       " 'chance',\n",
       " 'play',\n",
       " 'bt',\n",
       " 'wk',\n",
       " 'didn',\n",
       " 'nd',\n",
       " 'offer',\n",
       " 'face',\n",
       " 'big',\n",
       " 'awarded',\n",
       " 'never',\n",
       " 'long',\n",
       " 'reach',\n",
       " 'thanx',\n",
       " 'world',\n",
       " 'kiss',\n",
       " 'watch',\n",
       " 'little',\n",
       " 'sir',\n",
       " 'into',\n",
       " 'fun',\n",
       " 'called',\n",
       " 'quite',\n",
       " 'orange',\n",
       " 'once',\n",
       " 'guys',\n",
       " 'cool',\n",
       " 'hrs',\n",
       " 'remember',\n",
       " 'house',\n",
       " 'landline',\n",
       " 'office',\n",
       " 'shall',\n",
       " 'month',\n",
       " 'maybe',\n",
       " 'watching',\n",
       " 'plan',\n",
       " 'lot',\n",
       " 'cost',\n",
       " 'easy',\n",
       " 'guess',\n",
       " 'sweet',\n",
       " 'ah',\n",
       " 'shows',\n",
       " 'start',\n",
       " 'enough',\n",
       " 'network',\n",
       " 'ever',\n",
       " 'probably',\n",
       " 'girl',\n",
       " 'liao',\n",
       " 'leh',\n",
       " 'nite',\n",
       " 'bad',\n",
       " 'hear',\n",
       " 'jus',\n",
       " 'pay',\n",
       " 'latest',\n",
       " 'speak',\n",
       " 'receive',\n",
       " 'code',\n",
       " 'rate',\n",
       " 'xx',\n",
       " 'word',\n",
       " 'afternoon',\n",
       " 'part',\n",
       " 'den',\n",
       " 'having',\n",
       " 'wont',\n",
       " 'baby',\n",
       " 'minutes',\n",
       " 'shopping',\n",
       " 'left',\n",
       " 'calls',\n",
       " 'thank',\n",
       " 'till',\n",
       " 'ringtone',\n",
       " 'anyway',\n",
       " 'pa',\n",
       " 'does',\n",
       " 'asked',\n",
       " 'two',\n",
       " 'aight',\n",
       " 'look',\n",
       " 'fuck',\n",
       " 'wanted',\n",
       " 'room',\n",
       " 'texts',\n",
       " 'bring',\n",
       " 'til',\n",
       " 'attempt',\n",
       " 'apply',\n",
       " 'everything',\n",
       " 'able',\n",
       " 'years',\n",
       " 'working',\n",
       " 'dunno',\n",
       " 'made',\n",
       " 'looking',\n",
       " 'put',\n",
       " 'entry',\n",
       " 'mind',\n",
       " 'times',\n",
       " 'princess',\n",
       " 'wife',\n",
       " 'weekend',\n",
       " 'came',\n",
       " 'though',\n",
       " 'wake',\n",
       " 'yours',\n",
       " 'collect',\n",
       " 'must',\n",
       " 'forgot',\n",
       " 'award',\n",
       " 'actually',\n",
       " 'update',\n",
       " 'hav',\n",
       " 'missed',\n",
       " 'havent',\n",
       " 'saw',\n",
       " 'video',\n",
       " 'okay',\n",
       " 'bed',\n",
       " 'join',\n",
       " 'sat',\n",
       " 'dis',\n",
       " 'boy',\n",
       " 'important',\n",
       " 'while',\n",
       " 'hour',\n",
       " 'hair',\n",
       " 'driving',\n",
       " 'between',\n",
       " 'bus',\n",
       " 'sexy',\n",
       " 'dude',\n",
       " 'town',\n",
       " 'most',\n",
       " 'shop',\n",
       " 'evening',\n",
       " 'tv',\n",
       " 'juz',\n",
       " 'xxx',\n",
       " 'goin',\n",
       " 'else',\n",
       " 'pain',\n",
       " 'abt',\n",
       " 'early',\n",
       " 'away',\n",
       " 'delivery',\n",
       " 'gift',\n",
       " 'collection',\n",
       " 'goes',\n",
       " 'luv',\n",
       " 'club',\n",
       " 'mail',\n",
       " 'age',\n",
       " 'wif',\n",
       " 'colour',\n",
       " 'yesterday',\n",
       " 'hours',\n",
       " 'haven',\n",
       " 'food',\n",
       " 'didnt',\n",
       " 'bored',\n",
       " 'minute',\n",
       " 'pics',\n",
       " 'guy',\n",
       " 'coz',\n",
       " 'details',\n",
       " 'hot',\n",
       " 'wants',\n",
       " 'points',\n",
       " 'worry',\n",
       " 'tmr',\n",
       " 'plus',\n",
       " 'tried',\n",
       " 'bonus',\n",
       " 'online',\n",
       " 'oso',\n",
       " 'messages',\n",
       " 'show',\n",
       " 'private',\n",
       " 'ard',\n",
       " 'says',\n",
       " 'drive',\n",
       " 'means',\n",
       " 'started',\n",
       " 'movie',\n",
       " 'carlos',\n",
       " 'open',\n",
       " 'xmas',\n",
       " 'book',\n",
       " 'sleeping',\n",
       " 'valid',\n",
       " 'address',\n",
       " 'these',\n",
       " 'test',\n",
       " 'optout',\n",
       " 'sae',\n",
       " 'tones',\n",
       " 'alright',\n",
       " 'took',\n",
       " 'since',\n",
       " 'mths',\n",
       " 'price',\n",
       " 'de',\n",
       " 'until',\n",
       " 'together',\n",
       " 'weekly',\n",
       " 'ring',\n",
       " 'run',\n",
       " 'expires',\n",
       " 'hurt',\n",
       " 'sounds',\n",
       " 'yourself',\n",
       " 'awesome',\n",
       " 'statement',\n",
       " 'pic',\n",
       " 'net',\n",
       " 'dad',\n",
       " 'game',\n",
       " 'pounds',\n",
       " 'sad',\n",
       " 'true',\n",
       " 'head',\n",
       " 'family',\n",
       " 'everyone',\n",
       " 'wid',\n",
       " 'lei',\n",
       " 'walk',\n",
       " 'beautiful',\n",
       " 'selected',\n",
       " 'mom',\n",
       " 'missing',\n",
       " 'trip',\n",
       " 'friendship',\n",
       " 'music',\n",
       " 'feeling',\n",
       " 'vouchers',\n",
       " 'full',\n",
       " 'nt',\n",
       " 'await',\n",
       " 'saying',\n",
       " 'date',\n",
       " 'tot',\n",
       " 'land',\n",
       " 'chikku',\n",
       " 'making',\n",
       " 'rite',\n",
       " 'decimal',\n",
       " 'college',\n",
       " 'tomo',\n",
       " 'school',\n",
       " 'identifier',\n",
       " 'question',\n",
       " 'sch',\n",
       " 'wen',\n",
       " 'reason',\n",
       " 'voucher',\n",
       " 'available',\n",
       " 'http',\n",
       " 'story',\n",
       " 'makes',\n",
       " 'wot',\n",
       " 'games',\n",
       " 'congrats',\n",
       " 'taking',\n",
       " 'haf',\n",
       " 'huh',\n",
       " 'row',\n",
       " 'without',\n",
       " 'smiling',\n",
       " 'lets',\n",
       " 'auction',\n",
       " 'frnd',\n",
       " 'company',\n",
       " 'answer',\n",
       " 'either',\n",
       " 'final',\n",
       " 'mum',\n",
       " 'post',\n",
       " 'lots',\n",
       " 'congratulations',\n",
       " 'decided',\n",
       " 'unsubscribe',\n",
       " 'alone',\n",
       " 'charge',\n",
       " 'double',\n",
       " 'un',\n",
       " 'card',\n",
       " 'national',\n",
       " 'wil',\n",
       " 'simple',\n",
       " 'set',\n",
       " 'content',\n",
       " 'knw',\n",
       " 'drink',\n",
       " 'listen',\n",
       " 'anyone',\n",
       " 'forget',\n",
       " 'dating',\n",
       " 'finished',\n",
       " 'services',\n",
       " 'angry',\n",
       " 'words',\n",
       " 'break',\n",
       " 'change',\n",
       " 'each',\n",
       " 'sister',\n",
       " 'treat',\n",
       " 'gr',\n",
       " 'mean',\n",
       " 'brother',\n",
       " 'email',\n",
       " 'gd',\n",
       " 'busy',\n",
       " 'id',\n",
       " 'telling',\n",
       " 'tc',\n",
       " 'order',\n",
       " 'old',\n",
       " 'neva',\n",
       " 'takes',\n",
       " 'cause',\n",
       " 'top',\n",
       " 'calling',\n",
       " 'lose',\n",
       " 'light',\n",
       " 'pretty',\n",
       " 'search',\n",
       " 'log',\n",
       " 'smoke',\n",
       " 'goodmorning',\n",
       " 'case',\n",
       " 'cut',\n",
       " 'blue',\n",
       " 'type',\n",
       " 'savamob',\n",
       " 'plz',\n",
       " 'mate',\n",
       " 'party',\n",
       " 'currently',\n",
       " 'mah',\n",
       " 'least',\n",
       " 'leaving',\n",
       " 'redeemed',\n",
       " 'close',\n",
       " 'stay',\n",
       " 'hows',\n",
       " 'lands',\n",
       " 'mob',\n",
       " 'joy',\n",
       " 'believe',\n",
       " 'chennai',\n",
       " 'noe',\n",
       " 'suite',\n",
       " 'msgs',\n",
       " 'okie',\n",
       " 'visit',\n",
       " 'freemsg',\n",
       " 'both',\n",
       " 'whats',\n",
       " 'bout',\n",
       " 'kind',\n",
       " 'ni',\n",
       " 'their',\n",
       " 'gas',\n",
       " 'fucking',\n",
       " 'hit',\n",
       " 'song',\n",
       " 'caller',\n",
       " 'touch',\n",
       " 'sunday',\n",
       " 'sex',\n",
       " 'frm',\n",
       " 'etc',\n",
       " 'area',\n",
       " 'comin',\n",
       " 'hmv',\n",
       " 'dogging',\n",
       " 'parents',\n",
       " 'park',\n",
       " 'meant',\n",
       " 'dreams',\n",
       " 'prob',\n",
       " 'gbp',\n",
       " 'fr',\n",
       " 'loving',\n",
       " 'fancy',\n",
       " 'direct',\n",
       " 'yr',\n",
       " 'sea',\n",
       " 'wonderful',\n",
       " 'boytoy',\n",
       " 'yar',\n",
       " 'worth',\n",
       " 'wit',\n",
       " 'those',\n",
       " 'eh',\n",
       " 'fast',\n",
       " 'happened',\n",
       " 'winner',\n",
       " 'friday',\n",
       " 'far',\n",
       " 'seen',\n",
       " 'tired',\n",
       " 'swing',\n",
       " 'darlin',\n",
       " 'read',\n",
       " 'lucky',\n",
       " 'mates',\n",
       " 'phones',\n",
       " 'hard',\n",
       " 'gone',\n",
       " 'xy',\n",
       " 'darren',\n",
       " 'paper',\n",
       " 'bank',\n",
       " 'second',\n",
       " 'ugh',\n",
       " 'finally',\n",
       " 'john',\n",
       " 'doesn',\n",
       " 'news',\n",
       " 'tho',\n",
       " 'opt',\n",
       " 'balance',\n",
       " 'smth',\n",
       " 'lesson',\n",
       " 'ass',\n",
       " 'secret',\n",
       " 'poly',\n",
       " 'welcome',\n",
       " 'checking',\n",
       " 'computer',\n",
       " 'small',\n",
       " 'unlimited',\n",
       " 'semester',\n",
       " 'enter',\n",
       " 'wkly',\n",
       " 'within',\n",
       " 'seeing',\n",
       " 'camcorder',\n",
       " 'gotta',\n",
       " 'cum',\n",
       " 'loved',\n",
       " 'thinks',\n",
       " 'father',\n",
       " 'mobiles',\n",
       " 'used',\n",
       " 'mobileupd',\n",
       " 'snow',\n",
       " 'mr',\n",
       " 'happiness',\n",
       " 'thinking',\n",
       " 'mrng',\n",
       " 'mayb',\n",
       " 'motorola',\n",
       " 'oredi',\n",
       " 'information',\n",
       " 'ipod',\n",
       " 'player',\n",
       " 'whenever',\n",
       " 'fri',\n",
       " 'remove',\n",
       " 'eve',\n",
       " 'sis',\n",
       " 'todays',\n",
       " 'choose',\n",
       " 'grins',\n",
       " 'earlier',\n",
       " 'wq',\n",
       " 'fone',\n",
       " 'lovely',\n",
       " 'outside',\n",
       " 'found',\n",
       " 'supposed',\n",
       " 'lost',\n",
       " 'comes',\n",
       " 'gave',\n",
       " 'rest',\n",
       " 'monday',\n",
       " 'ltd',\n",
       " 'course',\n",
       " 'picking',\n",
       " 'hee',\n",
       " 'credit',\n",
       " 'hmmm',\n",
       " 'needs',\n",
       " 'talking',\n",
       " 'side',\n",
       " 'hg',\n",
       " 'hold',\n",
       " 'hl',\n",
       " 'hmm',\n",
       " 'mu',\n",
       " 'std',\n",
       " 'info',\n",
       " 'wine',\n",
       " 'using',\n",
       " 'pub',\n",
       " 'project',\n",
       " 'nope',\n",
       " 'police',\n",
       " 'knows',\n",
       " 'comp',\n",
       " 'comuk',\n",
       " 'happen',\n",
       " 'worried',\n",
       " 'ends',\n",
       " 'weed',\n",
       " 'û_',\n",
       " 'lovable',\n",
       " 'wrong',\n",
       " 'truth',\n",
       " 'anytime',\n",
       " 'wasn',\n",
       " 'complimentary',\n",
       " 'support',\n",
       " 'almost',\n",
       " 'abiola',\n",
       " 'reached',\n",
       " 'leaves',\n",
       " 'christmas',\n",
       " 'sun',\n",
       " 'sitting',\n",
       " 'la',\n",
       " 'point',\n",
       " 'discount',\n",
       " 'sk',\n",
       " 'quiz',\n",
       " 'del',\n",
       " 'mine',\n",
       " 'girls',\n",
       " 'valued',\n",
       " 'askd',\n",
       " 'felt',\n",
       " 'somebody',\n",
       " 'hungry',\n",
       " 'drop',\n",
       " 'gettin',\n",
       " 'yep',\n",
       " 'operator',\n",
       " 'sick',\n",
       " 'saturday',\n",
       " 'luck',\n",
       " 'tel',\n",
       " 'own',\n",
       " 'aft',\n",
       " 'fantastic',\n",
       " 'months',\n",
       " 'india',\n",
       " 'rs',\n",
       " 'txts',\n",
       " 'immediately',\n",
       " 'invited',\n",
       " 'rental',\n",
       " 'poor',\n",
       " 'march',\n",
       " 'buying',\n",
       " 'whatever',\n",
       " 'safe',\n",
       " 'yrs',\n",
       " 'asking',\n",
       " 'sending',\n",
       " 'south',\n",
       " 'bak',\n",
       " 'isn',\n",
       " 'planning',\n",
       " 'realy',\n",
       " 'sound',\n",
       " 'weeks',\n",
       " 'uncle',\n",
       " 'wow',\n",
       " 'correct',\n",
       " 'facebook',\n",
       " 'via',\n",
       " 'couple',\n",
       " 'wc',\n",
       " 'slowly',\n",
       " 'ten',\n",
       " 'children',\n",
       " 'press',\n",
       " 'goto',\n",
       " 'crave',\n",
       " 'ex',\n",
       " 'ntt',\n",
       " 'hw',\n",
       " 'convey',\n",
       " 'ha',\n",
       " 'kate',\n",
       " 'oops',\n",
       " 'txting',\n",
       " 'urself',\n",
       " 'jay',\n",
       " 'kids',\n",
       " 'wonder',\n",
       " 'die',\n",
       " 'plans',\n",
       " 'dnt',\n",
       " 'asap',\n",
       " 'spend',\n",
       " 'store',\n",
       " 'ac',\n",
       " 'warm',\n",
       " 'link',\n",
       " 'na',\n",
       " 'cr',\n",
       " 'extra',\n",
       " 'sort',\n",
       " 'stupid',\n",
       " 'hiya',\n",
       " 'drugs',\n",
       " 'nyt',\n",
       " 'usf',\n",
       " 'gym',\n",
       " 'bathe',\n",
       " 'shower',\n",
       " 'download',\n",
       " 'hand',\n",
       " 'terms',\n",
       " 'heard',\n",
       " 'sell',\n",
       " 'representative',\n",
       " 'catch',\n",
       " 'slow',\n",
       " 'mon',\n",
       " 'credits',\n",
       " 'hospital',\n",
       " 'confirm',\n",
       " 'others',\n",
       " 'mark',\n",
       " 'wondering',\n",
       " 'reading',\n",
       " 'across',\n",
       " 'frnds',\n",
       " 'nobody',\n",
       " 'access',\n",
       " 'offers',\n",
       " 'ûò',\n",
       " 'bath',\n",
       " 'doctor',\n",
       " 'future',\n",
       " 'ringtones',\n",
       " 'cold',\n",
       " 'sight',\n",
       " 'fact',\n",
       " 'forever',\n",
       " 'decide',\n",
       " 'running',\n",
       " 'seems',\n",
       " 'however',\n",
       " 'figure',\n",
       " 'yijue',\n",
       " 'trust',\n",
       " 'somewhere',\n",
       " 'exam',\n",
       " 'entitled',\n",
       " 'charged',\n",
       " 'opinion',\n",
       " 'nah',\n",
       " 'situation',\n",
       " 'max',\n",
       " 'unless',\n",
       " 'moment',\n",
       " 'studying',\n",
       " 'tampa',\n",
       " 'ice',\n",
       " 'hoping',\n",
       " 'rent',\n",
       " 'loan',\n",
       " 'pete',\n",
       " 'promise',\n",
       " 'coffee',\n",
       " 'feels',\n",
       " 'freephone',\n",
       " 'hurts',\n",
       " 'swt',\n",
       " 'style',\n",
       " 'possible',\n",
       " 'through',\n",
       " 'knew',\n",
       " 'replying',\n",
       " 'kinda',\n",
       " 'aha',\n",
       " 'recently',\n",
       " 'photo',\n",
       " 'marriage',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of words the word2vec model learned word vectors for\n",
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1672,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# We are getting a nested set of arrays inside an array of all the word vectors for each word in a text\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m w2v_vect \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([np\u001b[39m.\u001b[39;49marray([w2v_model\u001b[39m.\u001b[39;49mwv[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m ls \u001b[39mif\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m w2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mindex_to_key]) \u001b[39mfor\u001b[39;49;00m ls \u001b[39min\u001b[39;49;00m X_test])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1672,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# We are getting a nested set of arrays inside an array of all the word vectors for each word in a text\n",
    "w2v_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in w2v_model.wv.index_to_key]) for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Length help\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(w2v_vect):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(X_test\u001b[39m.\u001b[39miloc[i]), \u001b[39mlen\u001b[39m(v))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_vect' is not defined"
     ]
    }
   ],
   "source": [
    "# Length help\n",
    "for i, v in enumerate(w2v_vect):\n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
