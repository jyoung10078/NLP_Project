{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of some stop words in English\n",
    "stopwords.words('english')[0:1000:25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Pulling in the data and looking at the top rows\n",
    "messages = pd.read_csv('/Users/jared/Downloads/Ex_Files_Adv_NLP_Python_ML/Exercise Files/data/spam.csv', encoding = 'latin-1')\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the data\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats on our data\n",
    "messages.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a lot more ham values than ham values\n",
    "messages['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data\n",
    "print('Number of nulls in label: {}'.format(messages['label'].isnull().sum()))\n",
    "print('Number of nulls in text: {}'.format(messages['text'].isnull().sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Changing the way the tables will be displayed so that we can read more of the data this time around\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "\n",
    "#To do this, we need to show python what punctuation looks like\n",
    "#This library has a package called \"puncatuation\" that we can use for this step\n",
    "import string\n",
    "\n",
    "#Showing the punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reason we do this is to remove noise from the data\n",
    "\"This message is spam\" == \"This message is spam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: remove_punct(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - splitting our sentences into a list of words\n",
    "import re\n",
    "\n",
    "# \\W+ will split a text wherever it sees one or more non-word characters (white space, special characters, etc.)\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# We apply our function and then lower case all our words because python is case sensitive\n",
    "messages['text_tokenized'] = messages['text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to remove the stop words\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "messages['text_nostop'] = messages['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "- This creates a document-term matrix; one row per document, one column per word in the corpus\n",
    "- Generates a weighting for each word/document pair intended to reflect how important a given word is to the document within the context of its frequency within a larger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Reformat Data\n",
    "mess = pd.read_csv('/Users/jared/Downloads/Ex_Files_Adv_NLP_Python_ML/Exercise Files/data/spam.csv', encoding = 'latin-1')\n",
    "mess = mess.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "mess.columns = [\"label\", \"text\"]\n",
    "mess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One function to pre-process the data\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting a basic TFIDF Vectorizer and view the results\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#This will clean the data, fit it in a vectorizer, then create our document form matrix\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect.fit_transform(mess['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
